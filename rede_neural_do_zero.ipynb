{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOBS684c1OuU35eaGqyS683",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeanCarlosBarbosa88/JeanCarlosBarbosa88/blob/main/rede_neural_do_zero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf # Import tensorflow\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "metadata": {
        "id": "dvxk-FNucy3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor()\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "wYy9MKCxevi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "images, etiquetas = next(dataiter)  # Change dataiter.next() to next(dataiter)\n",
        "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')"
      ],
      "metadata": {
        "id": "pC9do5mmf9pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (images[0].shape)#para verificar as dimensão do tensor de cada imagem\n",
        "print (etiquetas[0].shape)#para verificar as dimensões do tensor de cada etiqueta"
      ],
      "metadata": {
        "id": "CarVS2dWiT1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "class Modelo(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Modelo, self).__init__()# camada de entrada, 784 neurônios que se ligam a 128\n",
        "        self.linear1 = nn.Linear(28*28, 128)# camada interna 1, 128 neurônios que se ligam a 64\n",
        "        self.linear2 = nn.Linear(128, 64)# camada interna 2, 64 neurônios que se ligam a 10\n",
        "        self.linear3 = nn.Linear(64, 10)#camada interna 2, 64 neurônios que se ligam a 10\n",
        "        #para camada de saida não e necessário definir nada pois só precisamos pegar o output da camada interna 2\n",
        "\n",
        "    def forward(self, x): # Ensure this line is indented at the same level as __init__\n",
        "        x = F.relu(self.linear1(x))# função de ativiaçãoda camada de entrada para camada interna 1\n",
        "        x = F.relu(self.linear2(x))# função de ativiaçãoda camada de entrada para camada interna 2\n",
        "        x = self.linear3(x)#função de ativação da camada interna 2 para camada de saida, nesse caso f(x)=x\n",
        "        return F.log_softmax(x, dim=1)#dados utilizados para calcular a perda"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "HBCQj5HomMIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treino(modelo, trainloader, device):\n",
        "    otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5)#otimizador\n",
        "    inicio = time()\n",
        "    criterio = nn.NLLLoss()#criterio de perda\n",
        "    EPOCHS = 10\n",
        "    modelo.train()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        perda_acumulada = 0.0\n",
        "        for images, etiquetas in trainloader:\n",
        "\n",
        "            images = images.view(images.shape[0], -1)\n",
        "            otimizador.zero_grad()\n",
        "            output = modelo(images.to(device))\n",
        "            perda_instantanea = criterio(output, etiquetas.to(device))\n",
        "            perda_instantanea.backward()\n",
        "            otimizador.step()\n",
        "            perda_acumulada += perda_instantanea.item()\n",
        "\n",
        "        else:\n",
        "            print(\"Treino: Epoch {} - Perda Computada (média): {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "            print(\"\\n Tempo de treino (em minutos) = \", (time()-inicio)/60)\n"
      ],
      "metadata": {
        "id": "03uBh_WMmhF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device):\n",
        "  conta_corretas, conta_todas = 0, 0\n",
        "  for images, etiquetas in valloader:\n",
        "      for i in range(len(etiquetas)):\n",
        "        img = images[i].view(1, 784)\n",
        "        # desativar o autoarmazenamento\n",
        "        with torch.no_grad():\n",
        "            logps = modelo(img.to(device))\n",
        "        ps = torch.exp(logps)\n",
        "        probab = list(ps.cpu().numpy()[0])\n",
        "        etiqueta_pred = probab.index(max(probab))\n",
        "        etiqueta_certa = etiquetas.numpy()[i]\n",
        "        if(etiqueta_certa == etiqueta_pred):\n",
        "            conta_corretas += 1\n",
        "            conta_todas += 1\n",
        "  print(\"Total de imagens testadas= {}/{}\".format(conta_corretas, conta_todas))\n",
        "  print(\"\\nPrecisão do modelo = {}%\".format(100 * conta_corretas / conta_todas))"
      ],
      "metadata": {
        "id": "d0o2sS_4opiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo.to(device)\n",
        "treino(modelo, trainloader, device)\n",
        "validacao(modelo, valloader, device)"
      ],
      "metadata": {
        "id": "yacP3wIKqSO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nova seção"
      ],
      "metadata": {
        "id": "ro3P0xXqbuX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Definindo a transformação (pré-processamento dos dados)\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Carregando os dados de treino e validação do MNIST\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Visualizando uma imagem de exemplo\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n",
        "plt.show()\n",
        "\n",
        "print(f'Tamanho da imagem: {images[0].shape}')  # Verificando as dimensões\n",
        "\n",
        "# Definindo o Modelo da Rede Neural (modelo simples de 3 camadas)\n",
        "class Modelo(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Modelo, self).__init__()\n",
        "        self.linear1 = nn.Linear(28*28, 128)  # Primeira camada\n",
        "        self.linear2 = nn.Linear(128, 64)     # Segunda camada\n",
        "        self.linear3 = nn.Linear(64, 10)      # Camada de saída (10 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Achata as imagens para 1D\n",
        "        x = F.relu(self.linear1(x))  # Função ReLU após a primeira camada\n",
        "        x = F.relu(self.linear2(x))  # Função ReLU após a segunda camada\n",
        "        x = self.linear3(x)  # Saída final sem ativação, pois vamos usar log_softmax\n",
        "        return F.log_softmax(x, dim=1)  # log_softmax para calcular a probabilidade\n",
        "\n",
        "# Função de treinamento\n",
        "def treino(modelo, trainloader, device):\n",
        "    otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5)\n",
        "    criterio = nn.NLLLoss()  # Função de perda de log-likelihood\n",
        "    EPOCHS = 6\n",
        "    modelo.train()\n",
        "\n",
        "    inicio = time()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        perda_acumulada = 0.0\n",
        "        for images, labels in trainloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            otimizador.zero_grad()\n",
        "            output = modelo(images)\n",
        "            perda = criterio(output, labels)\n",
        "            perda.backward()\n",
        "            otimizador.step()\n",
        "            perda_acumulada += perda.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Perda média: {perda_acumulada/len(trainloader):.4f}\")\n",
        "\n",
        "    print(f'Tempo de treino: {(time()-inicio)/60:.2f} minutos')\n",
        "\n",
        "# Função de validação\n",
        "def validacao(modelo, valloader, device):\n",
        "    modelo.eval()  # Coloca o modelo em modo de avaliação\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # Desabilita o cálculo de gradientes para a avaliação\n",
        "        for images, labels in valloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            output = modelo(images)\n",
        "            _, predicted = torch.max(output, 1)  # Obtém a classe com maior probabilidade\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Precisão no conjunto de validação: {100 * correct / total:.2f}%')\n",
        "\n",
        "#Função para carregar e processar imagem\n",
        "def carregar_imagem (imagem_path):\n",
        "  img = Image.open(imagem_path).convert('L')#converte para escala de cinza\n",
        "  transform = transforms.Compose([\n",
        "      transforms.Resize((28, 28)),   #redimensiona para 28x28 pixels\n",
        "      transforms.ToTensor(),       #converte para tensor\n",
        "      transforms.Normalize((0.5,), (0.5,))  #normalizção do MNIST\n",
        "  ])\n",
        "  img_tensor = transform(img)\n",
        "  img_tensor = img_tensor.unsqueeze(0)\n",
        "  return img_tensor\n",
        "\n",
        "#função para revisão\n",
        "def prever_imagem(modelo, imagem_tensor, device):\n",
        "  imagem_tensor = carregar_imagem(imagem_path)\n",
        "  imagem_tensor = imagem_tensor.to(device)\n",
        "\n",
        "  modelo.eval() # coloca em avaliação o modelo\n",
        "  with torch.no_grad():\n",
        "    output = modelo(imagem_tensor)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    print(f'A imagem é prevista como: {predicted.item()}')\n",
        "\n",
        "    #exibição da imagem\n",
        "    img = image.open('/content/bases-20240704T183510Z-001.zip').convert('L')\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title(f'Predição: {predicted.item()}')\n",
        "    plt.show()\n",
        "\n",
        "modelo = Modelo\n",
        "\n",
        "# Definindo o dispositivo (GPU ou CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo = Modelo().to(device)\n",
        "\n",
        "# Chamando as funções de treinamento e validação\n",
        "treino(modelo, trainloader, device)\n",
        "validacao(modelo, valloader, device)\n",
        "\n",
        "#testar nova imagem\n",
        "imagem_path = '/content/bases-20240704T183510Z-001.zip'\n",
        "prever_imagem(modelo, imagem_path, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        },
        "id": "9OZCqUlZGmUH",
        "outputId": "0d687d86-d2be-4b50-cb95-0e130d5ad289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho da imagem: torch.Size([1, 28, 28])\n",
            "Epoch 1, Perda média: 0.7097\n",
            "Epoch 2, Perda média: 0.3152\n",
            "Epoch 3, Perda média: 0.2611\n",
            "Epoch 4, Perda média: 0.2227\n",
            "Epoch 5, Perda média: 0.1908\n",
            "Epoch 6, Perda média: 0.1655\n",
            "Tempo de treino: 1.82 minutos\n",
            "Precisão no conjunto de validação: 94.80%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnidentifiedImageError",
          "evalue": "cannot identify image file '/content/bases-20240704T183510Z-001.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-8977cdc4ac31>\u001b[0m in \u001b[0;36m<cell line: 125>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m#testar nova imagem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mimagem_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/bases-20240704T183510Z-001.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mprever_imagem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimagem_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-84-8977cdc4ac31>\u001b[0m in \u001b[0;36mprever_imagem\u001b[0;34m(modelo, imagem_tensor, device)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m#função para revisão\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprever_imagem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimagem_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0mimagem_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcarregar_imagem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagem_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m   \u001b[0mimagem_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimagem_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-8977cdc4ac31>\u001b[0m in \u001b[0;36mcarregar_imagem\u001b[0;34m(imagem_path)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m#Função para carregar e processar imagem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcarregar_imagem\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimagem_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagem_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#converte para escala de cinza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m   transform = transforms.Compose([\n\u001b[1;32m     88\u001b[0m       \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m#redimensiona para 28x28 pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3534\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3535\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3536\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mUnidentifiedImageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '/content/bases-20240704T183510Z-001.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n"
      ],
      "metadata": {
        "id": "R5bBprTSIA7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Para 3 canais (RGB)\n"
      ],
      "metadata": {
        "id": "hfSQBU_UcQV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(7*7*64, 128)  # Aqui o número depende da imagem após convolução\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 7*7*64)  # Flattening\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "XZzBwHxXcSYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform)\n"
      ],
      "metadata": {
        "id": "g5TPvIlOcUg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "ujpq3UYXcXQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "otimizador = optim.Adam(modelo.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "ISXDd3JOcacD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterio = nn.CrossEntropyLoss()  # Para problemas com múltiplas classes"
      ],
      "metadata": {
        "id": "F3aWRcVFccii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10  # Altere o número conforme necessário"
      ],
      "metadata": {
        "id": "GO-wUbzncklD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h0JZwbAGki12"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}